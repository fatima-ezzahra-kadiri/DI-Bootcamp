{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebaf841d",
   "metadata": {
    "id": "ebaf841d"
   },
   "source": [
    "# Exercises XP: Student Notebook\n",
    "\n",
    "For each exercise, the **Instructions** from the plateform are guided, and the **Guidance** explains exactly what you must do to complete the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c0f244",
   "metadata": {
    "id": "f2c0f244"
   },
   "source": [
    "## What you will learn\n",
    "- How to clearly define and articulate a machine learning problem statement.\n",
    "\n",
    "- The process of data collection, including identifying relevant data types and potential data sources.\n",
    "Skills in feature selection and justification for machine learning models, particularly in the context of loan default prediction.\n",
    "\n",
    "- Understanding of different types of machine learning models and their suitability for various real-world scenarios.\n",
    "\n",
    "- Techniques and strategies for evaluating the performance of different machine learning models, including choosing appropriate metrics and understanding their implications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aa83e3",
   "metadata": {
    "id": "17aa83e3"
   },
   "source": [
    "## What you will create\n",
    "- A detailed problem statement and data collection plan for a loan default prediction project, including identification of key data types and sources.\n",
    "- A comprehensive feature selection analysis for a hypothetical loan default prediction dataset.\n",
    "- A theoretical evaluation strategy for three different types of machine learning models, addressing the unique challenges and metrics relevant to each model type.\n",
    "- Thoughtful analyses and justifications for choosing specific machine learning approaches for varied scenarios such as stock price prediction, library organization, and robot navigation.\n",
    "- A document or presentation that showcases your understanding and approach to evaluating and optimizing machine learning models in diverse contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e965f9",
   "metadata": {
    "id": "93e965f9"
   },
   "source": [
    "## ðŸŒŸ Exercise 1 : Defining the Problem and Data Collection for Loan Default Prediction\n",
    "\n",
    "### Instructions\n",
    "- Write a clear problem statement for predicting loan defaults.\n",
    "- Identify and list the types of data you would need for this project (e.g., personal details of applicants, credit scores, loan amounts, repayment history).\n",
    "- Discuss the sources where you can collect this data (e.g., financial institutionâ€™s internal records, credit bureaus).\n",
    "\n",
    "**Expected Output:** A document detailing the problem statement and a comprehensive plan for data collection, including data types and sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ea95bd",
   "metadata": {
    "id": "62ea95bd"
   },
   "source": [
    "### Guidance\n",
    "- Please write your answer as a short document. Begin by stating the prediction objective in a complete sentence that names the target variable and the decision it will support. Then, describe the data types you would collect in complete sentences. For each data type, explain in one sentence why it could help predict loan defaults.\n",
    "\n",
    "- After that, name realistic data sources in complete sentences, and briefly describe how you would obtain or integrate each source.\n",
    "\n",
    "- Finally, include one paragraph that explains risks and constraints such as privacy, regulation, data quality, sampling bias, and governance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b165d7a",
   "metadata": {
    "id": "9b165d7a"
   },
   "source": [
    "### Your answer\n",
    "> The objectif of this project is to predict whether a loan applicant will default on their loan, using historical, personal, and financial information. The target variable is a binary indicator of default(Yes/No), and the prediction will support risk-based decision-making for loan approvals.\n",
    "\n",
    "> To build this model, several categories of data are required, including personal and socio-economic details such as age, gender, marital status, number of dependents, employment status, income level, and region of residence, as these variables help estimte financial stability and repayment capacity. Additional fiancial and credit data such as credit scores, existing debts, monthly financial obligations, and if available, bank account history, are essential to evaluate creditworthiness and financial discipline. Loan specific attributes, including loan amount, duration, interest rate, and loan purpose, further influence the risk assiciated with the credit request. Finally, repayment history, including past late payments, or previous defaults, provides one of the strongest predictors of future repayment behavior.\n",
    "\n",
    "> These data can be collected from institutional records (client portfolios, repayment logs, past applications), customer provided application forms, and public socio-economic datasets that complement demographic and regional information.\n",
    "\n",
    "> Working with personal and financial data introduces critical risks and constraints that must be managed carefully. The institution must comply with privacy and data protection regulations, such as CNDP guidelines and general rules governing personnaly indentifiable information. Potnetial challenges include data quality issues, missing or inconsistent values, biased historical decisions that may unintentionally propagate discrimination, and sampling bias that could distort the model's predictions. strong data governance is therefore essential, ensuring secure storage, restricted access, anonymization where appropriate, and ethical handling of sensitive data throughout the entire model lifecycle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c297407",
   "metadata": {
    "id": "7c297407"
   },
   "source": [
    "## ðŸŒŸ Exercise 2 : Feature Selection and Model Choice for Loan Default Prediction\n",
    "\n",
    "### Instructions\n",
    "From this dataset, identify which features might be most relevant for predicting loan defaults.\n",
    "Justify your choice of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deb8c95",
   "metadata": {
    "id": "4deb8c95"
   },
   "source": [
    "### Guidance\n",
    "- First, identify the features that you believe are most relevant, and write their names in a sentence.\n",
    "Then, provide a justification in complete sentences that explains how each selected feature relates to the likelihood of default.\n",
    "\n",
    "- If you decide to exclude common features, write one sentence for each excluded feature to explain why it is not appropriate in this context.\n",
    "\n",
    "- Conclude with two complete sentences that explain how you would encode categorical features and how you would impute missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3af9600b",
   "metadata": {
    "id": "3af9600b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will now justify the selected features in complete sentences below.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This piece of code is already prefilled, run it to execute it and see the results.\n",
    "# It provides a simple template you can modify while writing your justification.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# This placeholder DataFrame allows the cell to run even if you did not load a dataset yet.\n",
    "example_columns = [\n",
    "    \"age\",\"employment_length\",\"annual_income\",\"credit_score\",\"loan_amount\",\"interest_rate\",\n",
    "    \"debt_to_income\",\"num_delinquencies\",\"num_open_accounts\",\"total_utilization\",\"home_ownership\",\n",
    "    \"purpose\",\"term\",\"application_type\",\"state\",\"zip_code\"\n",
    "]\n",
    "df = pd.DataFrame(columns=example_columns)\n",
    "\n",
    "# Please replace this list with the actual columns that you select.\n",
    "selected_features = [\n",
    "    # e.g., \"credit_score\",\"debt_to_income\",\"annual_income\",\"loan_amount\",\"interest_rate\",\n",
    "    # \"employment_length\",\"num_delinquencies\",\"total_utilization\"\n",
    "]\n",
    "\n",
    "print(\"You will now justify the selected features in complete sentences below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0e3a74",
   "metadata": {
    "id": "ed0e3a74"
   },
   "source": [
    "### Your justification\n",
    "> the features that I believe are more relevant are: Applicant Income (that mesure the revenue of the applicant), Loan Amount, Credit history, self employed, \n",
    "\n",
    "Also explain, in two complete sentences, how you would encode categorical variables and how you would impute missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4822cf-c1a4-43f9-b6d7-28de5640c178",
   "metadata": {},
   "source": [
    "### Feature Selection and Model Choice for Loan Default Prediction\n",
    "\n",
    "> The most relevant features for predicting loan default are ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount_Term, Credit_History, Dependents, Education, Self_Employed, Married, Property_Area, and Gender. Income-related features such as ApplicantIncome and CoapplicantIncome help evaluate the borrowerâ€™s repayment capacity. LoanAmount and Loan_Amount_Term are directly tied to affordability and monthly repayment pressure. Credit_History is one of the strongest indicators of financial discipline and past borrower behavior. Additionally, demographic and socio-economic attributes like Dependents, Education, Married status, Gender, and Self_Employed provide valuable context on financial responsibilities, job stability, and the applicantâ€™s overall repayment reliability. Property_Area may also influence default risk due to economic variations between urban, semi-urban, and rural zones.\n",
    "\n",
    "> Loan_ID is excluded from modeling because it is only an identifier and contains no predictive value. Loan_Status is not considered a feature since it represents the target variable that the model aims to predict.\n",
    "\n",
    "> Categorical variables such as Gender, Married, Education, Self_Employed, and Property_Area will be encoded using one-hot encoding or label encoding, depending on the needs of the algorithm. Missing values will be handled through median imputation for numerical variables and mode imputation for categorical variables, ensuring consistent and reliable data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e6708",
   "metadata": {
    "id": "9e8e6708"
   },
   "source": [
    "## ðŸŒŸ Exercise 3 : Training, Evaluating, and Optimizing the Model\n",
    "\n",
    "### Instructions\n",
    "Which model(s) would you pick for a Loan Prediction ?\n",
    "Outline the steps to evaluate the modelâ€™s performance, mentioning specific metrics that would be relevant to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe22b7ce",
   "metadata": {
    "id": "fe22b7ce"
   },
   "source": [
    "### Guidance\n",
    "- Begin by naming one or two candidate models in a complete sentence and explain why each model is suitable for this problem.\n",
    "\n",
    "- Next, describe an evaluation plan in complete sentences that covers the data split, the cross-validation strategy, the metrics you will report, and how you will choose a decision threshold.\n",
    "\n",
    "- Then, explain in complete sentences how you will address class imbalance using stratification, class weights, or resampling.\n",
    "\n",
    "- Finally, state in one or two complete sentences how you would iterate on hyperparameters to improve performance while avoiding data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d630ee52",
   "metadata": {
    "id": "d630ee52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1-score: 1.0\n",
      "ROC-AUC: 1.0\n",
      "PR-AUC (Average Precision): 1.0\n",
      "\n",
      "Confusion matrix:\n",
      " [[6 0]\n",
      " [0 4]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         6\n",
      "           1       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           1.00        10\n",
      "   macro avg       1.00      1.00      1.00        10\n",
      "weighted avg       1.00      1.00      1.00        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This piece of code is already prefilled, run it to execute it and see the results.\n",
    "# It demonstrates standard classification metrics for binary loan default prediction.\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix, classification_report\n",
    "\n",
    "# Please replace these placeholders with your true labels and predicted probabilities.\n",
    "y_true = [0,1,0,1,0,0,1,0,1,0]            # placeholder labels\n",
    "y_pred_proba = [0.05,0.80,0.10,0.65,0.20,0.15,0.70,0.30,0.85,0.25]  # placeholder probabilities\n",
    "\n",
    "# You should set a decision threshold that reflects the precisionâ€“recall trade-off for your business case.\n",
    "threshold = 0.5\n",
    "y_pred = [1 if p >= threshold else 0 for p in y_pred_proba]\n",
    "\n",
    "print(\"Accuracy:\", round(accuracy_score(y_true, y_pred), 4))\n",
    "print(\"Precision:\", round(precision_score(y_true, y_pred, zero_division=0), 4))\n",
    "print(\"Recall:\", round(recall_score(y_true, y_pred, zero_division=0), 4))\n",
    "print(\"F1-score:\", round(f1_score(y_true, y_pred, zero_division=0), 4))\n",
    "print(\"ROC-AUC:\", round(roc_auc_score(y_true, y_pred_proba), 4))\n",
    "print(\"PR-AUC (Average Precision):\", round(average_precision_score(y_true, y_pred_proba), 4))\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_true, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd4054c",
   "metadata": {
    "id": "cdd4054c"
   },
   "source": [
    "### Your answer\n",
    "> For the loan default prediction task, I would choose Logistic Regression and Random Forest as candidate models. Logistic Regression is suitable because it provides interpretable coefficients and performs well on binary classification problems with structured data. Random Forest is also appropriate because it captures non-linear relationships, handles mixed feature types, and is robust to noise and missing values.\n",
    "\n",
    "> To evaluate the models, I would first split the dataset into training and test sets using a stratified split to preserve the proportion of default vs. non-default cases. I would then apply cross-validation, such as 5-fold stratified cross-validation, to obtain stable performance estimates. The main metrics I would report are recall, precision, F1-score, ROC-AUC, and the confusion matrix, since predicting defaults is a high-risk decision. I would also tune and interpret the decision threshold by analyzing the precision-recall tradeoff to reduce false negatives, which are costly in loan approval scenarios.\n",
    "\n",
    "> To address class imbalance, I would use stratified sampling during splitting and cross-validation, and I would test both class weights and resampling techniques such as SMOTE or undersampling. These methods help ensure the model does not become biased toward the majority class.\n",
    "\n",
    "> Finally, I would optimize performance by tuning hyperparameters through GridSearchCV or RandomizedSearchCV, ensuring that all preprocessing steps occur inside a pipeline to avoid data leakage. Hyperparameter search would be repeated only on the training folds to maintain fairness and validity of the evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0488eb67",
   "metadata": {
    "id": "0488eb67"
   },
   "source": [
    "## ðŸŒŸ Exercise 4 : Designing Machine Learning Solutions for Specific Problems\n",
    "\n",
    "### Instructions\n",
    "For each of these scenario, decide which type of machine learning would be most suitable. Explain.\n",
    "\n",
    "Predicting Stock Prices : predict future prices\n",
    "Organizing a Library of Books : group books into genres or categories based on similarities.\n",
    "Program a robot to navigate and find the shortest path in a maze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c00056",
   "metadata": {
    "id": "00c00056"
   },
   "source": [
    "### Guidance\n",
    "Please identify the appropriate machine learning paradigm for each scenario in complete sentences and justify your choice.\n",
    "\n",
    "For each scenario, write one complete sentence that describes the input data, one complete sentence that describes the output, and one complete sentence that describes the learning signal or objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27bd365",
   "metadata": {
    "id": "d27bd365"
   },
   "source": [
    "### Your answer\n",
    "#### 1. Predicting Stock Prices\n",
    "\n",
    "Type: Supervised learning (regression).\n",
    "\n",
    "Input: Historical stock prices and financial indicators.\n",
    "\n",
    "Output: A numerical prediction of the future stock price.\n",
    "\n",
    "Learning signal: The model minimizes the error between predicted and actual prices.\n",
    "\n",
    "#### 2. Organizing a Library of Books\n",
    "\n",
    "Type: Unsupervised learning (clustering).\n",
    "\n",
    "Input: Text descriptions or metadata of books.\n",
    "\n",
    "Output: Groups of similar books.\n",
    "\n",
    "Learning signal: The model identifies natural patterns without labels.\n",
    "\n",
    "#### 3. Robot Navigating a Maze\n",
    "\n",
    "Type: Reinforcement learning.\n",
    "\n",
    "Input: The robotâ€™s current state and surrounding environment.\n",
    "\n",
    "Output: The next action the robot should take.\n",
    "\n",
    "Learning signal: Rewards for actions that lead to faster or successful navigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8e44a7",
   "metadata": {
    "id": "2e8e44a7"
   },
   "source": [
    "## ðŸŒŸ Exercise 5 : Designing an Evaluation Strategy for Different ML Models\n",
    "\n",
    "### Instructions\n",
    "- Select three types of machine learning models: one from supervised learning (e.g., a classification model), one from unsupervised learning (e.g., a clustering model), and one from reinforcement learning. - For the supervised model, outline a strategy to evaluate its performance, including the choice of metrics (like accuracy, precision, recall, F1-score) and methods (like cross-validation, ROC curves).\n",
    "- For the unsupervised model, describe how you would assess the effectiveness of the model, considering techniques like silhouette score, elbow method, or cluster validation metrics.\n",
    "- For the reinforcement learning model, discuss how you would measure its success, considering aspects like cumulative reward, convergence, and exploration vs. exploitation balance.\n",
    "- Address the challenges and limitations of evaluating models in each category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f13dd9",
   "metadata": {
    "id": "f6f13dd9"
   },
   "source": [
    "### Guidance\n",
    "- Please write a separate paragraph for each of the three model categories.\n",
    "- In the supervised paragraph, describe your validation plan and list the metrics you will report in complete sentences.\n",
    "- In the unsupervised paragraph, explain how you would measure cluster quality or structure in complete sentences and mention any diagnostic plots.\n",
    "- In the reinforcement learning paragraph, describe how you would track cumulative reward, assess convergence, and balance exploration and exploitation using complete sentences.\n",
    "Conclude with one complete sentence per category that states a key evaluation challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff6d0257",
   "metadata": {
    "id": "ff6d0257"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1-score: 1.0\n",
      "ROC-AUC: 1.0\n",
      "PR-AUC (Average Precision): 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This piece of code is already prefilled, run it to execute it and see the results.\n",
    "# Supervised classification metrics template with placeholders.\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "# Replace these placeholders with your real outputs.\n",
    "y_true = [0,1,1,0,1,0,0,1,0,1]\n",
    "y_pred_proba = [0.1,0.7,0.8,0.2,0.6,0.3,0.4,0.9,0.2,0.85]\n",
    "threshold = 0.5\n",
    "y_pred = [1 if p >= threshold else 0 for p in y_pred_proba]\n",
    "\n",
    "print(\"Accuracy:\", round(accuracy_score(y_true, y_pred), 4))\n",
    "print(\"Precision:\", round(precision_score(y_true, y_pred, zero_division=0), 4))\n",
    "print(\"Recall:\", round(recall_score(y_true, y_pred, zero_division=0), 4))\n",
    "print(\"F1-score:\", round(f1_score(y_true, y_pred, zero_division=0), 4))\n",
    "print(\"ROC-AUC:\", round(roc_auc_score(y_true, y_pred_proba), 4))\n",
    "print(\"PR-AUC (Average Precision):\", round(average_precision_score(y_true, y_pred_proba), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf3c51e0",
   "metadata": {
    "id": "cf3c51e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score (higher is better): 0.848\n",
      "Please explain in complete sentences when you would use the elbow method and how you would interpret it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This piece of code is already prefilled, run it to execute it and see the results.\n",
    "# Unsupervised clustering metrics template with synthetic data.\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "X, _ = make_blobs(n_samples=300, centers=3, random_state=42)\n",
    "kmeans = KMeans(n_clusters=3, n_init=\"auto\", random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "sil = silhouette_score(X, labels)\n",
    "print(\"Silhouette score (higher is better):\", round(sil, 4))\n",
    "\n",
    "print(\"Please explain in complete sentences when you would use the elbow method and how you would interpret it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d9f0ef",
   "metadata": {
    "id": "e7d9f0ef"
   },
   "source": [
    "### Your answer\n",
    "#### Supervised Learning Model (Classification)\n",
    "> To evaluate a supervised classification model, I would begin by splitting the data into training and test sets and applying cross-validation to ensure stable performance across folds. I would report accuracy, precision, recall, and F1-score to capture both overall correctness and class-specific performance, and I would plot the ROC curve and compute the AUC to evaluate ranking quality. I would also examine the confusion matrix to identify systematic classification errors. A key challenge in evaluating supervised models is dealing with class imbalance, which can make accuracy misleading.\n",
    "\n",
    "#### Unsupervised Learning Model (Clustering)\n",
    ">To assess an unsupervised clustering model, I would measure cluster structure using metrics such as the silhouette score to quantify separation and cohesion. I would also use the elbow method to inspect how the within-cluster variance changes with different numbers of clusters and rely on diagnostic plots like silhouette diagrams or inertia curves to visually judge cluster quality. If ground truth labels exist, I could additionally compute adjusted Rand index or mutual information. A major challenge in evaluating unsupervised models is the absence of true labels, which makes objective evaluation difficult.\n",
    "\n",
    "#### Reinforcement Learning Model\n",
    ">To evaluate a reinforcement learning model, I would track cumulative reward over episodes to measure how effectively the agent learns to maximize long-term returns. I would check for convergence by observing whether the learning curve stabilizes and ensure an appropriate explorationâ€“exploitation balance by monitoring how often the agent explores new actions versus exploiting known profitable ones. I would also test the learned policy in multiple environments or seeds to ensure stability and robustness. A key challenge in evaluating reinforcement learning models is that performance can vary widely depending on stochastic environments and exploration behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9f5d92-843d-4407-b1be-fc6ea6bde012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
